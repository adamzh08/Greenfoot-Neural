<!DOCTYPE HTML>
<html lang="de">
<head>
<!-- Generated by javadoc (21) on Wed May 14 18:09:48 CEST 2025 -->
<title>ActivationFunctions (Greenfoot-Neural)</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="dc.created" content="2025-05-14">
<meta name="description" content="declaration: class: ActivationFunctions">
<meta name="generator" content="javadoc/ClassWriterImpl">
<link rel="stylesheet" type="text/css" href="stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="script-dir/jquery-ui.min.css" title="Style">
<script type="text/javascript" src="script.js"></script>
<script type="text/javascript" src="script-dir/jquery-3.6.1.min.js"></script>
<script type="text/javascript" src="script-dir/jquery-ui.min.js"></script>
</head>
<body class="class-declaration-page">
<script type="text/javascript">var pathtoroot = "./";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript ist im Browser deaktiviert.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top"><button id="navbar-toggle-button" aria-controls="navbar-top" aria-expanded="false" aria-label="Navigationslinks umschalten"><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span></button>
<div class="skip-nav"><a href="#skip-navbar-top" title="Navigations-Links überspringen">Navigations-Links überspringen</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="package-summary.html">Package</a></li>
<li class="nav-bar-cell1-rev">Klasse</li>
<li><a href="package-tree.html">Baum</a></li>
<li><a href="index-all.html">Index</a></li>
<li><a href="help-doc.html#class">Hilfe</a></li>
</ul>
<ul class="sub-nav-list-small">
<li>
<p>Übersicht:</p>
<ul>
<li>Verschachtelt</li>
<li>Feld</li>
<li>Konstruktor</li>
<li><a href="#method-summary">Methode</a></li>
</ul>
</li>
<li>
<p>Details:</p>
<ul>
<li>Feld</li>
<li>Konstruktor</li>
<li><a href="#method-detail">Methode</a></li>
</ul>
</li>
</ul>
</div>
<div class="sub-nav">
<div id="navbar-sub-list">
<ul class="sub-nav-list">
<li>Übersicht:&nbsp;</li>
<li>Verschachtelt&nbsp;|&nbsp;</li>
<li>Feld&nbsp;|&nbsp;</li>
<li>Konstruktor&nbsp;|&nbsp;</li>
<li><a href="#method-summary">Methode</a></li>
</ul>
<ul class="sub-nav-list">
<li>Details:&nbsp;</li>
<li>Feld&nbsp;|&nbsp;</li>
<li>Konstruktor&nbsp;|&nbsp;</li>
<li><a href="#method-detail">Methode</a></li>
</ul>
</div>
<div class="nav-list-search"><a href="search.html">SEARCH</a>
<input type="text" id="search-input" disabled placeholder="Suchen">
<input type="reset" id="reset-button" disabled value="reset">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="flex-content">
<main role="main">
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<h1 title="Klasse ActivationFunctions" class="title">Klasse ActivationFunctions</h1>
</div>
<div class="inheritance" title="Vererbungsbaum"><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html" title="Klasse oder Schnittstelle in java.lang" class="external-link">java.lang.Object</a>
<div class="inheritance">ActivationFunctions</div>
</div>
<section class="class-description" id="class-description">
<hr>
<div class="type-signature"><span class="modifiers">public final class </span><span class="element-name type-name-label">ActivationFunctions</span>
<span class="extends-implements">extends <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html" title="Klasse oder Schnittstelle in java.lang" class="external-link">Object</a></span></div>
<div class="block">Collection of activation functions for neural networks</div>
</section>
<section class="summary">
<ul class="summary-list">
<!-- ========== METHOD SUMMARY =========== -->
<li>
<section class="method-summary" id="method-summary">
<h2>Methodenübersicht</h2>
<div id="method-summary-table">
<div class="table-tabs" role="tablist" aria-orientation="horizontal"><button id="method-summary-table-tab0" role="tab" aria-selected="true" aria-controls="method-summary-table.tabpanel" tabindex="0" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table', 3)" class="active-table-tab">Alle Methoden</button><button id="method-summary-table-tab1" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab1', 3)" class="table-tab">Statische Methoden</button><button id="method-summary-table-tab4" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab4', 3)" class="table-tab">Konkrete Methoden</button></div>
<div id="method-summary-table.tabpanel" role="tabpanel" aria-labelledby="method-summary-table-tab0">
<div class="summary-table three-column-summary">
<div class="table-header col-first">Modifizierer und Typ</div>
<div class="table-header col-second">Methode</div>
<div class="table-header col-last">Beschreibung</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#elu(double)" class="member-name-link">elu</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">ELU with default alpha value</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#elu(double,double)" class="member-name-link">elu</a><wbr>(double&nbsp;x,
 double&nbsp;alpha)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Exponential Linear Unit activation function

 Characteristics: - Smooth function including at x=0 - Can produce
 negative values - Better handling of noise - Self-regularizing</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#gelu(double)" class="member-name-link">gelu</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Gaussian Error Linear Unit (GELU) activation

 Characteristics: - Smooth approximation of ReLU - Used in modern
 transformers - Combines properties of dropout and ReLU - More
 computationally expensive</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#linear(double)" class="member-name-link">linear</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">&nbsp;</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#lrelu(double)" class="member-name-link">lrelu</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Leaky ReLU with default alpha value</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#lrelu(double,double)" class="member-name-link">lrelu</a><wbr>(double&nbsp;x,
 double&nbsp;alpha)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Leaky ReLU activation function

 Characteristics: - Prevents dying ReLU problem - Small gradient for
 negative values - No vanishing gradient</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#prelu(double,double)" class="member-name-link">prelu</a><wbr>(double&nbsp;x,
 double&nbsp;alpha)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Parametric ReLU activation function

 Characteristics: - Similar to Leaky ReLU but with learnable alpha - More
 flexible than standard ReLU - Requires additional parameter training</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#relu(double)" class="member-name-link">relu</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Rectified Linear Unit (ReLU) activation function

 Characteristics: - Simple and computationally efficient - No vanishing
 gradient for positive values - Can cause "dying ReLU" problem - Most
 commonly used activation in modern networks</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#sigmoid(double)" class="member-name-link">sigmoid</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Sigmoid activation function

 Characteristics: - Smooth, continuous function - Output range: (0,1) -
 Commonly used in binary classification - Can cause vanishing gradient
 problems</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static void</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#softmax(double%5B%5D,double%5B%5D,int)" class="member-name-link">softmax</a><wbr>(double[]&nbsp;input,
 double[]&nbsp;output,
 int&nbsp;size)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Softmax activation function for entire layer

 Characteristics: - Converts inputs to probability distribution - Outputs
 sum to 1.0 - Commonly used in classification - Numerically stable
 implementation</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#softmaxSingle(double)" class="member-name-link">softmaxSingle</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Single-input softmax for network structure

 Note: This is only part of the softmax calculation.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#tanh(double)" class="member-name-link">tanh</a><wbr>(double&nbsp;x)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">Hyperbolic tangent activation function

 Characteristics: - Zero-centered output - Output range: (-1,1) - Stronger
 gradients than sigmoid - Still can have vanishing gradient issues</div>
</div>
</div>
</div>
</div>
<div class="inherited-list">
<h3 id="methods-inherited-from-class-java.lang.Object">Von Klasse geerbte Methoden&nbsp;java.lang.<a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html" title="Klasse oder Schnittstelle in java.lang" class="external-link">Object</a></h3>
<code><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#clone()" title="Klasse oder Schnittstelle in java.lang" class="external-link">clone</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#equals(java.lang.Object)" title="Klasse oder Schnittstelle in java.lang" class="external-link">equals</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#getClass()" title="Klasse oder Schnittstelle in java.lang" class="external-link">getClass</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#hashCode()" title="Klasse oder Schnittstelle in java.lang" class="external-link">hashCode</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#notify()" title="Klasse oder Schnittstelle in java.lang" class="external-link">notify</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#notifyAll()" title="Klasse oder Schnittstelle in java.lang" class="external-link">notifyAll</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#toString()" title="Klasse oder Schnittstelle in java.lang" class="external-link">toString</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#wait()" title="Klasse oder Schnittstelle in java.lang" class="external-link">wait</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#wait(long)" title="Klasse oder Schnittstelle in java.lang" class="external-link">wait</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#wait(long,int)" title="Klasse oder Schnittstelle in java.lang" class="external-link">wait</a></code></div>
</section>
</li>
</ul>
</section>
<section class="details">
<ul class="details-list">
<!-- ============ METHOD DETAIL ========== -->
<li>
<section class="method-details" id="method-detail">
<h2>Methodendetails</h2>
<ul class="member-list">
<li>
<section class="detail" id="linear(double)">
<h3>linear</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">linear</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
</section>
</li>
<li>
<section class="detail" id="sigmoid(double)">
<h3>sigmoid</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">sigmoid</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
<div class="block">Sigmoid activation function

 Characteristics: - Smooth, continuous function - Output range: (0,1) -
 Commonly used in binary classification - Can cause vanishing gradient
 problems</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dt>Gibt zurück:</dt>
<dd>Output in range (0,1)</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="tanh(double)">
<h3>tanh</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">tanh</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
<div class="block">Hyperbolic tangent activation function

 Characteristics: - Zero-centered output - Output range: (-1,1) - Stronger
 gradients than sigmoid - Still can have vanishing gradient issues</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dt>Gibt zurück:</dt>
<dd>Output in range (-1,1)</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="relu(double)">
<h3>relu</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">relu</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
<div class="block">Rectified Linear Unit (ReLU) activation function

 Characteristics: - Simple and computationally efficient - No vanishing
 gradient for positive values - Can cause "dying ReLU" problem - Most
 commonly used activation in modern networks</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dt>Gibt zurück:</dt>
<dd>max(0,x)</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="lrelu(double,double)">
<h3>lrelu</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">lrelu</span><wbr><span class="parameters">(double&nbsp;x,
 double&nbsp;alpha)</span></div>
<div class="block">Leaky ReLU activation function

 Characteristics: - Prevents dying ReLU problem - Small gradient for
 negative values - No vanishing gradient</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dd><code>alpha</code> - Slope for negative values (typically small, e.g., 0.01)</dd>
<dt>Gibt zurück:</dt>
<dd>x if x &gt; 0, alpha * x otherwise</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="lrelu(double)">
<h3>lrelu</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">lrelu</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
<div class="block">Leaky ReLU with default alpha value</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dt>Gibt zurück:</dt>
<dd>x if x &gt; 0, 0.01 * x otherwise</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="prelu(double,double)">
<h3>prelu</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">prelu</span><wbr><span class="parameters">(double&nbsp;x,
 double&nbsp;alpha)</span></div>
<div class="block">Parametric ReLU activation function

 Characteristics: - Similar to Leaky ReLU but with learnable alpha - More
 flexible than standard ReLU - Requires additional parameter training</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dd><code>alpha</code> - Learnable parameter for negative values</dd>
<dt>Gibt zurück:</dt>
<dd>x if x &gt; 0, alpha * x otherwise</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="elu(double,double)">
<h3>elu</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">elu</span><wbr><span class="parameters">(double&nbsp;x,
 double&nbsp;alpha)</span></div>
<div class="block">Exponential Linear Unit activation function

 Characteristics: - Smooth function including at x=0 - Can produce
 negative values - Better handling of noise - Self-regularizing</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dd><code>alpha</code> - Scale for the negative part</dd>
<dt>Gibt zurück:</dt>
<dd>x if x ≥ 0, alpha * (exp(x) - 1) otherwise</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="elu(double)">
<h3>elu</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">elu</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
<div class="block">ELU with default alpha value</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dt>Gibt zurück:</dt>
<dd>x if x ≥ 0, (exp(x) - 1) otherwise</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="softmaxSingle(double)">
<h3>softmaxSingle</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">softmaxSingle</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
<div class="block">Single-input softmax for network structure

 Note: This is only part of the softmax calculation. Full normalization
 happens in the network forward pass.</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dt>Gibt zurück:</dt>
<dd>Exponential of input (partial softmax)</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="softmax(double[],double[],int)">
<h3>softmax</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">void</span>&nbsp;<span class="element-name">softmax</span><wbr><span class="parameters">(double[]&nbsp;input,
 double[]&nbsp;output,
 int&nbsp;size)</span></div>
<div class="block">Softmax activation function for entire layer

 Characteristics: - Converts inputs to probability distribution - Outputs
 sum to 1.0 - Commonly used in classification - Numerically stable
 implementation</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>input</code> - Array of input values</dd>
<dd><code>output</code> - Array to store results</dd>
<dd><code>size</code> - Length of input/output arrays</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="gelu(double)">
<h3>gelu</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double</span>&nbsp;<span class="element-name">gelu</span><wbr><span class="parameters">(double&nbsp;x)</span></div>
<div class="block">Gaussian Error Linear Unit (GELU) activation

 Characteristics: - Smooth approximation of ReLU - Used in modern
 transformers - Combines properties of dropout and ReLU - More
 computationally expensive</div>
<dl class="notes">
<dt>Parameter:</dt>
<dd><code>x</code> - Input value</dd>
<dt>Gibt zurück:</dt>
<dd>GELU activation value</dd>
</dl>
</section>
</li>
</ul>
</section>
</li>
</ul>
</section>
<!-- ========= END OF CLASS DATA ========= -->
</main>
</div>
</div>
</body>
</html>
