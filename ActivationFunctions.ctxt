#BlueJ class context
comment0.target=ActivationFunctions
comment0.text=\r\n\ Collection\ of\ activation\ functions\ for\ neural\ networks\r\n
comment1.params=
comment1.target=ActivationFunctions()
comment10.params=x
comment10.target=double\ elu(double)
comment10.text=\r\n\ ELU\ with\ default\ alpha\ value\r\n\r\n\ @param\ x\ Input\ value\r\n\ @return\ x\ if\ x\ \u2265\ 0,\ (exp(x)\ -\ 1)\ otherwise\r\n
comment11.params=x
comment11.target=double\ softmaxSingle(double)
comment11.text=\r\n\ Single-input\ softmax\ for\ network\ structure\r\n\r\n\ Note\:\ This\ is\ only\ part\ of\ the\ softmax\ calculation.\ Full\ normalization\r\n\ happens\ in\ the\ network\ forward\ pass.\r\n\r\n\ @param\ x\ Input\ value\r\n\ @return\ Exponential\ of\ input\ (partial\ softmax)\r\n
comment12.params=input\ output\ size
comment12.target=void\ softmax(double[],\ double[],\ int)
comment12.text=\r\n\ Softmax\ activation\ function\ for\ entire\ layer\r\n\r\n\ Characteristics\:\ -\ Converts\ inputs\ to\ probability\ distribution\ -\ Outputs\r\n\ sum\ to\ 1.0\ -\ Commonly\ used\ in\ classification\ -\ Numerically\ stable\r\n\ implementation\r\n\r\n\ @param\ input\ \ Array\ of\ input\ values\r\n\ @param\ output\ Array\ to\ store\ results\r\n\ @param\ size\ \ \ Length\ of\ input/output\ arrays\r\n
comment13.params=x
comment13.target=double\ gelu(double)
comment13.text=\r\n\ Gaussian\ Error\ Linear\ Unit\ (GELU)\ activation\r\n\r\n\ Characteristics\:\ -\ Smooth\ approximation\ of\ ReLU\ -\ Used\ in\ modern\r\n\ transformers\ -\ Combines\ properties\ of\ dropout\ and\ ReLU\ -\ More\r\n\ computationally\ expensive\r\n\r\n\ @param\ x\ Input\ value\r\n\ @return\ GELU\ activation\ value\r\n
comment2.params=x
comment2.target=double\ linear(double)
comment3.params=x
comment3.target=double\ sigmoid(double)
comment3.text=\r\n\ Sigmoid\ activation\ function\r\n\r\n\ Characteristics\:\ -\ Smooth,\ continuous\ function\ -\ Output\ range\:\ (0,1)\ -\r\n\ Commonly\ used\ in\ binary\ classification\ -\ Can\ cause\ vanishing\ gradient\r\n\ problems\r\n\r\n\ @param\ x\ Input\ value\r\n\ @return\ Output\ in\ range\ (0,1)\r\n
comment4.params=x
comment4.target=double\ tanh(double)
comment4.text=\r\n\ Hyperbolic\ tangent\ activation\ function\r\n\r\n\ Characteristics\:\ -\ Zero-centered\ output\ -\ Output\ range\:\ (-1,1)\ -\ Stronger\r\n\ gradients\ than\ sigmoid\ -\ Still\ can\ have\ vanishing\ gradient\ issues\r\n\r\n\ @param\ x\ Input\ value\r\n\ @return\ Output\ in\ range\ (-1,1)\r\n
comment5.params=x
comment5.target=double\ relu(double)
comment5.text=\r\n\ Rectified\ Linear\ Unit\ (ReLU)\ activation\ function\r\n\r\n\ Characteristics\:\ -\ Simple\ and\ computationally\ efficient\ -\ No\ vanishing\r\n\ gradient\ for\ positive\ values\ -\ Can\ cause\ "dying\ ReLU"\ problem\ -\ Most\r\n\ commonly\ used\ activation\ in\ modern\ networks\r\n\r\n\ @param\ x\ Input\ value\r\n\ @return\ max(0,x)\r\n
comment6.params=x\ alpha
comment6.target=double\ lrelu(double,\ double)
comment6.text=\r\n\ Leaky\ ReLU\ activation\ function\r\n\r\n\ Characteristics\:\ -\ Prevents\ dying\ ReLU\ problem\ -\ Small\ gradient\ for\r\n\ negative\ values\ -\ No\ vanishing\ gradient\r\n\r\n\ @param\ x\ \ \ \ \ Input\ value\r\n\ @param\ alpha\ Slope\ for\ negative\ values\ (typically\ small,\ e.g.,\ 0.01)\r\n\ @return\ x\ if\ x\ >\ 0,\ alpha\ *\ x\ otherwise\r\n
comment7.params=x
comment7.target=double\ lrelu(double)
comment7.text=\r\n\ Leaky\ ReLU\ with\ default\ alpha\ value\r\n\r\n\ @param\ x\ Input\ value\r\n\ @return\ x\ if\ x\ >\ 0,\ 0.01\ *\ x\ otherwise\r\n
comment8.params=x\ alpha
comment8.target=double\ prelu(double,\ double)
comment8.text=\r\n\ Parametric\ ReLU\ activation\ function\r\n\r\n\ Characteristics\:\ -\ Similar\ to\ Leaky\ ReLU\ but\ with\ learnable\ alpha\ -\ More\r\n\ flexible\ than\ standard\ ReLU\ -\ Requires\ additional\ parameter\ training\r\n\r\n\ @param\ x\ \ \ \ \ Input\ value\r\n\ @param\ alpha\ Learnable\ parameter\ for\ negative\ values\r\n\ @return\ x\ if\ x\ >\ 0,\ alpha\ *\ x\ otherwise\r\n
comment9.params=x\ alpha
comment9.target=double\ elu(double,\ double)
comment9.text=\r\n\ Exponential\ Linear\ Unit\ activation\ function\r\n\r\n\ Characteristics\:\ -\ Smooth\ function\ including\ at\ x\=0\ -\ Can\ produce\r\n\ negative\ values\ -\ Better\ handling\ of\ noise\ -\ Self-regularizing\r\n\r\n\ @param\ x\ \ \ \ \ Input\ value\r\n\ @param\ alpha\ Scale\ for\ the\ negative\ part\r\n\ @return\ x\ if\ x\ \u2265\ 0,\ alpha\ *\ (exp(x)\ -\ 1)\ otherwise\r\n
numComments=14
